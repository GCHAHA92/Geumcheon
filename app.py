import streamlit as st
from pymongo import MongoClient
from pydantic import BaseModel
from openai import OpenAI
from pdfminer.high_level import extract_text
import json
import io
import re

# -----------------------------
# ê¸°ë³¸ ì„¤ì •
# -----------------------------
st.set_page_config(layout="wide", page_title="ê°ì‚¬ê²°ê³¼ PDF íŒŒì¼ íŒŒì‹± ì„œë¹„ìŠ¤")
st.title("ê°ì‚¬ê²°ê³¼ PDF ìë™ êµ¬ì¡°í™” ì‹œìŠ¤í…œ")

# -----------------------------
# ì‹œí¬ë¦¿ ë¡œë”©
# -----------------------------
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY")
MONGO_URI = st.secrets.get("MONGO_URI")

if not OPENAI_API_KEY or not MONGO_URI:
    st.error("OPENAI_API_KEY ë˜ëŠ” MONGO_URIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .streamlit/secrets.tomlì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# -----------------------------
# í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
# -----------------------------
client = OpenAI(api_key=OPENAI_API_KEY)
mongo_client = MongoClient(MONGO_URI)
db = mongo_client["json_db"]
collection = db["Yangsan_Audit"]

# -----------------------------
# Pydantic ëª¨ë¸
# -----------------------------
class AuditResult(BaseModel):
    ê±´ëª…: str
    ì²˜ë¶„: str
    ê´€ë ¨ê·œì •: str
    ì§€ì ì‚¬í•­: str

class ResearchPaperExtraction(BaseModel):
    ê°ì‚¬ì—°ë„: str
    í”¼ê°ê¸°ê´€: str
    ê°ì‚¬ê²°ê³¼: list[AuditResult]

# -----------------------------
# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜
# -----------------------------
def extract_text_from_pdf(file):
    if hasattr(file, "read"):
        data = file.read()
        file.seek(0)
        return extract_text(io.BytesIO(data))
    return extract_text(file)

# -----------------------------
# í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜ (í‘œÂ·ì—°ë²ˆ ì œê±°)
# -----------------------------
def clean_text_for_ai(text: str) -> str:
    lines = text.splitlines()
    cleaned = []

    for line in lines:
        # í‘œ êµ¬ì¡°ë‚˜ êµ¬ë¶„ì„  ì œê±°
        if re.search(r"[â”‚â”ƒâ”â”“â”—â”›â”â•\-]{3,}", line):  # ê¸´ êµ¬ë¶„ì„ 
            continue
        if re.search(r"^\s*\d{1,2}\s*[.|)]", line):  # ì—°ë²ˆ (1. / 2) / 3)
            continue
        if "í‘œ " in line or "í‘œ-" in line or "table" in line.lower():
            continue
        if len(line.strip()) == 0:
            continue

        # ê¸ˆì•¡ì´ë‚˜ ì´ê±´ìˆ˜ëŠ” ìœ ì§€ (ì˜ˆ: 27,000ì› / ì´ 14ê±´)
        cleaned.append(line)

    return "\n".join(cleaned)

# -----------------------------
# ì„¸ì…˜ ìƒíƒœ
# -----------------------------
if "extracted_text" not in st.session_state:
    st.session_state["extracted_text"] = None
if "structured_json" not in st.session_state:
    st.session_state["structured_json"] = None

# -----------------------------
# ë ˆì´ì•„ì›ƒ
# -----------------------------
col1, col2 = st.columns(2)

# ----------- (1) íŒŒì¼ ì—…ë¡œë“œ -----------
with col1:
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf")
    if uploaded_file:
        extracted_text = extract_text_from_pdf(uploaded_file)
        st.session_state["extracted_text"] = extracted_text

        st.subheader("ğŸ“„ PDF ì›ë¬¸ ë¯¸ë¦¬ë³´ê¸°")
        st.text_area("ì¶”ì¶œëœ í…ìŠ¤íŠ¸", extracted_text[:8000], height=400)

# ----------- (2) AI ë¶„ì„ -----------
with col2:
    if st.session_state.get("extracted_text"):
        cleaned_text = clean_text_for_ai(st.session_state["extracted_text"])

        if st.button("AIë¡œ êµ¬ì¡°í™”(JSON) ë³€í™˜"):
            with st.spinner("AIê°€ ë¬¸ì„œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    completion = client.beta.chat.completions.parse(
                        model="gpt-4o-mini",
                        messages=[
                            {
                                "role": "system",
                                "content": (
                                    "You are an expert in audit report parsing. "
                                    "You must convert unstructured text into structured JSON according to the schema."
                                ),
                            },
                            {
                                "role": "user",
                                "content": (
                                    f"{cleaned_text}\n\n"
                                    "ë‹¤ìŒ ì¡°ê±´ì„ ì§€ì¼œ ê°ì‚¬ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ êµ¬ì¡°í™”í•˜ì„¸ìš”:\n"
                                    "- 'ì‹œì •','ì£¼ì˜','ê¸°íƒ€','íšŒìˆ˜(í™˜ìˆ˜)','ì¶”ê¸‰(í™˜ê¸‰)','ì§•ê³„','í›ˆê³„(ê²½ì§•ê³„/ì¤‘ì§•ê³„)' ì²˜ë¶„ê²°ê³¼ë¥¼ ëª¨ë‘ í¬í•¨í•©ë‹ˆë‹¤.\n"
                                    "- í‘œ, ì—°ë²ˆ, ëª©ë¡í˜• ë°ì´í„°(1. 2. 3. â€¦)ëŠ” ì œê±°í•©ë‹ˆë‹¤.\n"
                                    "- ê¸ˆì•¡(ì˜ˆ: 27,000ì›), ì´ ê±´ìˆ˜(ì˜ˆ: ì´ 14ê±´)ëŠ” ìœ ì§€í•©ë‹ˆë‹¤.\n"
                                    "- ê´€ë ¨ê·œì •ì€ ìš”ì•½í•˜ì§€ ë§ê³  ë²•ë ¹ ì›ë¬¸ ì „ì²´ë¥¼ ê·¸ëŒ€ë¡œ í¬í•¨í•©ë‹ˆë‹¤.\n"
                                    "- ì¡°ì¹˜í•  ì‚¬í•­ì€ ë°˜ë“œì‹œ í¬í•¨í•©ë‹ˆë‹¤.\n"
                                    "- JSON í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n"
                                    "{ 'ê°ì‚¬ì—°ë„': str, 'í”¼ê°ê¸°ê´€': str, 'ê°ì‚¬ê²°ê³¼': [ {'ê±´ëª…': str, 'ì²˜ë¶„': str, 'ê´€ë ¨ê·œì •': str, 'ì§€ì ì‚¬í•­': str} ] }"
                                ),
                            },
                        ],
                        response_format=ResearchPaperExtraction,
                        temperature=0,
                    )

                    structured = completion.choices[0].message.parsed
                    st.session_state["structured_json"] = structured

                    st.success("âœ… AI êµ¬ì¡°í™” ì™„ë£Œ!")
                    st.json(structured.model_dump())

                except Exception as e:
                    st.error(f"AI ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        if st.session_state.get("structured_json"):
            if st.button("MongoDB ì €ì¥"):
                doc = st.session_state["structured_json"].model_dump()
                collection.insert_one(doc)
                st.success("âœ… MongoDBì— ì €ì¥ ì™„ë£Œ!")

# ----------- (3) ê²€ìƒ‰ -----------
st.markdown("---")
st.subheader("MongoDB ê²€ìƒ‰")

search_query = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
if search_query:
    query = {
        "ê°ì‚¬ê²°ê³¼": {
            "$elemMatch": {
                "$or": [
                    {"ê±´ëª…": {"$regex": search_query, "$options": "i"}},
                    {"ì²˜ë¶„": {"$regex": search_query, "$options": "i"}},
                    {"ê´€ë ¨ê·œì •": {"$regex": search_query, "$options": "i"}},
                    {"ì§€ì ì‚¬í•­": {"$regex": search_query, "$options": "i"}},
                ]
            }
        }
    }

    results = list(collection.find(query))
    if results:
        st.success(f"ì´ {len(results)}ê±´ì˜ ê²°ê³¼ê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
        for idx, doc in enumerate(results, start=1):
            st.markdown(f"### {idx}. {doc.get('í”¼ê°ê¸°ê´€')} ({doc.get('ê°ì‚¬ì—°ë„')})")
            for r in doc.get("ê°ì‚¬ê²°ê³¼", []):
                st.markdown(f"**ê±´ëª…:** {r.get('ê±´ëª…')}  \n**ì²˜ë¶„:** {r.get('ì²˜ë¶„')}  \n**ê´€ë ¨ê·œì •:** {r.get('ê´€ë ¨ê·œì •')}  \n**ì§€ì ì‚¬í•­:** {r.get('ì§€ì ì‚¬í•­')}")
                st.markdown("---")
    else:
        st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
