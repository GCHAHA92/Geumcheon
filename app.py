import streamlit as st
from pymongo import MongoClient
from pydantic import BaseModel
from openai import OpenAI
from pdfminer.high_level import extract_text
import json
import io
import re

# -----------------------------
# ê¸°ë³¸ ì„¤ì •
# -----------------------------
st.set_page_config(layout="wide", page_title="ê°ì‚¬ê²°ê³¼ PDF íŒŒì¼ íŒŒì‹± ì„œë¹„ìŠ¤")
st.title("ê°ì‚¬ê²°ê³¼ PDF ìë™ êµ¬ì¡°í™” ì‹œìŠ¤í…œ")

# -----------------------------
# ì‹œí¬ë¦¿ ë¡œë”©
# -----------------------------
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY")
MONGO_URI = st.secrets.get("MONGO_URI")

if not OPENAI_API_KEY or not MONGO_URI:
    st.error("OPENAI_API_KEY ë˜ëŠ” MONGO_URIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .streamlit/secrets.tomlì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# -----------------------------
# í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
# -----------------------------
client = OpenAI(api_key=OPENAI_API_KEY)
mongo_client = MongoClient(MONGO_URI)
db = mongo_client["json_db"]
collection = db["Yangsan_Audit"]

# -----------------------------
# Pydantic ëª¨ë¸
# -----------------------------
class AuditResult(BaseModel):
    ë¶„ì•¼: str | None = None   # ìƒˆ í•„ë“œ
    ê±´ëª…: str
    ì²˜ë¶„: str
    ê´€ë ¨ê·œì •: str
    ì§€ì ì‚¬í•­: str

class ResearchPaperExtraction(BaseModel):
    ê°ì‚¬ì—°ë„: str
    í”¼ê°ê¸°ê´€: str
    ê°ì‚¬ê²°ê³¼: list[AuditResult]

# -----------------------------
# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜
# -----------------------------
def extract_text_from_pdf(file):
    if hasattr(file, "read"):
        data = file.read()
        file.seek(0)
        return extract_text(io.BytesIO(data))
    return extract_text(file)

# -----------------------------
# í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜ (í‘œÂ·ì—°ë²ˆ ì œê±°)
# -----------------------------
def clean_text_for_ai(text: str) -> str:
    lines = text.splitlines()
    cleaned = []

    for line in lines:
        raw = line
        line = line.rstrip("\n")

        # 1) ì™„ì „í•œ êµ¬ë¶„ì„ (í…Œì´ë¸” í…Œë‘ë¦¬ ë“±) ì œê±°
        #    â”€, â”‚, â”ƒ, â”, â”“, â”—, â”›, =, - ë“±ìœ¼ë¡œë§Œ ì´ë£¨ì–´ì§„ ì¤„
        if re.match(r"^[\sâ”‚â”ƒâ”â”“â”—â”›â”â•\-_=]+$", line):
            continue

        stripped = line.strip()

        # 2) ì™„ì „íˆ ë¹„ì–´ ìˆëŠ” ì¤„ì€ ê±´ë„ˆë›°ê¸°
        if not stripped:
            continue

        # 3) í˜ì´ì§€ ë²ˆí˜¸ í˜•ì‹ ì œê±° (ì˜ˆ: "- 15 -", "15 / 32" ë“±)
        if re.match(r"^[\-â€“â€”\s]*\d+\s*/\s*\d+[\-â€“â€”\s]*$", stripped):
            continue
        if re.match(r"^[\-â€“â€”\s]*\d+[\-â€“â€”\s]*$", stripped) and len(stripped) <= 8:
            # ì§§ì€ í˜ì´ì§€ ë²ˆí˜¸ í˜•íƒœ(ì˜ˆ: "- 15 -", "15")ë§Œ ì œê±°
            continue

        # 4) í‘œ ìº¡ì…˜ ì œê±° (ì˜ˆ: "í‘œ 1", "í‘œ 2-1", "Table 1" ë“±)
        if re.match(r"^í‘œ\s*\d+([\--â€“]\d+)?", stripped):
            continue
        if "table" in stripped.lower():
            continue

        # 5) ë¦¬ìŠ¤íŠ¸ ë²ˆí˜¸ ê°™ì€ "1.", "2)", "3. ê°€)" í˜•íƒœëŠ” ì œê±°í•˜ë˜
        #    ì‹¤ì œ ì œëª©/ê±´ëª… ì¤„ì€ ì ˆëŒ€ ì‚­ì œí•˜ì§€ ì•Šê¸°
        #
        #   - ì˜ˆ) "1." / "2)" / "3. ê°€)" ì²˜ëŸ¼ ìˆ«ì+ê¸°í˜¸ë§Œ ìˆê³  ë‚´ìš©ì´ ê±°ì˜ ì—†ëŠ” ê²½ìš°ë§Œ ì œê±°
        #
        if re.match(r"^\d{1,2}\s*[.)]\s*$", stripped):
            # ë‚´ìš© ì—†ëŠ” ìˆœë²ˆë§Œ ìˆëŠ” ì¤„ (ì˜ˆ: "1." / "2)")
            continue
        if re.match(r"^\d{1,2}\s*[.)]\s*[ê°€-í£]\s*$", stripped):
            # ì˜ˆ: "1. ê°€" "2) ë‚˜" ê°™ì€ ìˆœë²ˆ+í•œ ê¸€ìë§Œ ìˆëŠ” ì¤„
            continue

        # â›” ì—¬ê¸°ì„œë¶€í„°ëŠ” "5. ê±´ê°•ê´€ë¦¬ ë¶„ì•¼", "15 â—‹â—‹ì„¼í„° ë¹„í’ˆê´€ë¦¬ëŒ€ì¥â€¦" ê°™ì€
        #    ì‹¤ì œ ì œëª©/ê±´ëª… ì¤„ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ë¨

        cleaned.append(line)

    return "\n".join(cleaned)


# -----------------------------
# ì„¸ì…˜ ìƒíƒœ
# -----------------------------
if "extracted_text" not in st.session_state:
    st.session_state["extracted_text"] = None
if "structured_json" not in st.session_state:
    st.session_state["structured_json"] = None

# -----------------------------
# ë ˆì´ì•„ì›ƒ
# -----------------------------
col1, col2 = st.columns(2)

# ----------- (1) íŒŒì¼ ì—…ë¡œë“œ -----------
with col1:
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf")
    if uploaded_file:
        extracted_text = extract_text_from_pdf(uploaded_file)
        st.session_state["extracted_text"] = extracted_text

        st.subheader("ğŸ“„ PDF ì›ë¬¸ ë¯¸ë¦¬ë³´ê¸°")
        st.text_area("ì¶”ì¶œëœ í…ìŠ¤íŠ¸", extracted_text[:800000], height=400)

# ----------- (2) AI ë¶„ì„ -----------
with col2:
    if st.session_state.get("extracted_text"):
        cleaned_text = clean_text_for_ai(st.session_state["extracted_text"])

        if st.button("AIë¡œ êµ¬ì¡°í™”(JSON) ë³€í™˜"):
            with st.spinner("AIê°€ ë¬¸ì„œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    completion = client.beta.chat.completions.parse(
    model="gpt-5-mini",
    messages=[
        {
            "role": "system",
            "content": (
                "You are an expert in Korean audit report parsing. "
                "You must convert unstructured text into structured JSON according to the schema."
            ),
        },
        {
            "role": "user",
            "content": (
                f"{cleaned_text}\n\n"
                "ë‹¤ìŒ ì¡°ê±´ì„ ì§€ì¼œ ê°ì‚¬ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ êµ¬ì¡°í™”í•˜ì„¸ìš”:\n"
                "1) ìƒìœ„ ì œëª©ê³¼ ì„¸ë¶€ ì œëª©ì„ êµ¬ë¶„í•˜ì„¸ìš”.\n"
                "   - 'â—‹â—‹ ë¶„ì•¼', 'ê±´ê°•ê´€ë¦¬ ë¶„ì•¼', 'ì˜ˆì‚°Â·íšŒê³„ ë¶„ì•¼'ì²˜ëŸ¼ 'ë¶„ì•¼'ë¡œ ëë‚˜ëŠ” ê²ƒì€ **ë¶„ì•¼**ì…ë‹ˆë‹¤.\n"
                "   - '15 â—‹â—‹â—‹â—‹ì„¼í„° ë¹„í’ˆê´€ë¦¬ëŒ€ì¥ ê´€ë¦¬ ì†Œí™€ [ì‹œì •]'ì²˜ëŸ¼ ë²ˆí˜¸ + ì œëª© + [ì²˜ë¶„] í˜•íƒœëŠ”\n"
                "     ë²ˆí˜¸ë¥¼ ì œì™¸í•œ ë¶€ë¶„ì„ **ê±´ëª…**ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"
                "2) JSON í•„ë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n"
                "   - 'ë¶„ì•¼': 'ì˜ˆì‚°Â·íšŒê³„', 'ê±´ê°•ê´€ë¦¬', 'ë³´ê±´ìœ„ìƒ' ë“± ìƒìœ„ ë¶„ì•¼ ì´ë¦„(ì˜ˆ: 'ê±´ê°•ê´€ë¦¬ ë¶„ì•¼' â†’ 'ê±´ê°•ê´€ë¦¬').\n"
                "   - 'ê±´ëª…': ê° ì§€ì ì‚¬í•­ì˜ êµ¬ì²´ì ì¸ ì œëª©\n"
                "       ì˜ˆ) 'íŠ¹ë³„íœ´ê°€ ì‚¬ìš© ê´€ë¦¬ ì†Œí™€', 'â—‹â—‹ì„¼í„° ë¹„í’ˆê´€ë¦¬ëŒ€ì¥ ê´€ë¦¬ ì†Œí™€' ë“±.\n"
                "       'ì˜ˆì‚°Â·íšŒê³„ ë¶„ì•¼', 'ê±´ê°•ê´€ë¦¬ ë¶„ì•¼'ì²˜ëŸ¼ ìƒìœ„ ì œëª©ì€ ê±´ëª…ì— ì ˆëŒ€ ë„£ì§€ ë§ˆì„¸ìš”.\n"
                "   - 'ì²˜ë¶„': 'ì‹œì •', 'ì£¼ì˜', 'í†µë³´', 'ì‹œì •/ì£¼ì˜/í†µë³´' ë“±.\n"
                "   - 'ê´€ë ¨ê·œì •': í•´ë‹¹ ì§€ì ì‚¬í•­ ì•„ë˜ 'ê´€ë ¨ê·œì •' í•­ëª© ì „ì²´ (ìš”ì•½ ê¸ˆì§€).\n"
                "   - 'ì§€ì ì‚¬í•­': í•´ë‹¹ ì§€ì ì‚¬í•­ ì•„ë˜ 'ì§€ì ì‚¬í•­' ë° 'ì¡°ì¹˜í•  ì‚¬í•­' ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•œ ë¬¸ë‹¨.\n"
                "3) JSON ì „ì²´ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n"
                "{ 'ê°ì‚¬ì—°ë„': str,\n"
                "  'í”¼ê°ê¸°ê´€': str,\n"
                "  'ê°ì‚¬ê²°ê³¼': [\n"
                "    { 'ë¶„ì•¼': str, 'ê±´ëª…': str, 'ì²˜ë¶„': str, 'ê´€ë ¨ê·œì •': str, 'ì§€ì ì‚¬í•­': str }, ...\n"
                "  ]\n"
                "}\n"
            ),
        },
    ],
    response_format=ResearchPaperExtraction,
    temperature=0,
)

                    structured = completion.choices[0].message.parsed
                    st.session_state["structured_json"] = structured

                    st.success("âœ… AI êµ¬ì¡°í™” ì™„ë£Œ!")
                    st.json(structured.model_dump())

                except Exception as e:
                    st.error(f"AI ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        if st.session_state.get("structured_json"):
            if st.button("MongoDB ì €ì¥"):
                doc = st.session_state["structured_json"].model_dump()
                collection.insert_one(doc)
                st.success("âœ… MongoDBì— ì €ì¥ ì™„ë£Œ!")

# ----------- (3) ê²€ìƒ‰ -----------
st.markdown("---")
st.subheader("MongoDB ê²€ìƒ‰")

search_query = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")

if search_query:
    regex = re.compile(search_query, re.IGNORECASE)

    query = {
        "ê°ì‚¬ê²°ê³¼": {
            "$elemMatch": {
                "$or": [
                    {"ê±´ëª…": {"$regex": search_query, "$options": "i"}},
                    {"ì²˜ë¶„": {"$regex": search_query, "$options": "i"}},
                    {"ê´€ë ¨ê·œì •": {"$regex": search_query, "$options": "i"}},
                    {"ì§€ì ì‚¬í•­": {"$regex": search_query, "$options": "i"}},
                ]
            }
        }
    }

    results = list(collection.find(query))

    # ğŸ”¹ ë¬¸ì„œ ì•ˆì—ì„œ ë‹¤ì‹œ í•­ëª©ë³„ í•„í„°ë§
    total_matched = 0
    display_blocks = []

    for doc in results:
        matched_items = []
        for r in doc.get("ê°ì‚¬ê²°ê³¼", []):
            text_fields = [
                r.get("ê±´ëª…", ""),
                r.get("ì²˜ë¶„", ""),
                r.get("ê´€ë ¨ê·œì •", ""),
                r.get("ì§€ì ì‚¬í•­", ""),
            ]
            if any(regex.search(str(t)) for t in text_fields):
                matched_items.append(r)

        if matched_items:
            total_matched += len(matched_items)
            display_blocks.append((doc, matched_items))

    if total_matched > 0:
        st.success(f"ì´ {total_matched}ê±´ì˜ ê²°ê³¼ê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
        for idx, (doc, items) in enumerate(display_blocks, start=1):
            st.markdown(f"### {idx}. {doc.get('í”¼ê°ê¸°ê´€')} ({doc.get('ê°ì‚¬ì—°ë„')})")
            for r in items:
                st.markdown(
                    f"**ê±´ëª…:** {r.get('ê±´ëª…')}  \n"
                    f"**ì²˜ë¶„:** {r.get('ì²˜ë¶„')}  \n"
                    f"**ê´€ë ¨ê·œì •:** {r.get('ê´€ë ¨ê·œì •')}  \n"
                    f"**ì§€ì ì‚¬í•­:** {r.get('ì§€ì ì‚¬í•­')}"
                )
                st.markdown("---")
    else:
        st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")